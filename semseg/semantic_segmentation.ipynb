{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "In this exercise we will implement a convolutional neural network for semantic segmentation.\n",
    "The goal of semantic segmentation is to classify the image on the pixel level, for each pixel\n",
    "we want to determine the class of the object which it belongs to.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cityscapes dataset\n",
    "\n",
    "[Cityscapes dataset](https://www.cityscapes-dataset.com/dataset-overview/) contains a diverse set of stereo video sequences recorded in street scenes from 50 different cities, with high quality pixel-level annotations. Dataset contains 2975 training and 500 validation images of size 2048x1024. Here we will use downsampled images of size 384x160. The original dataset has \n",
    "\n",
    "0-road\n",
    "1-building\n",
    "2-infrastructure\n",
    "3-nature\n",
    "4-sky\n",
    "5-person\n",
    "6-vehicle\n",
    "7-ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the graph\n",
    "\n",
    "Let's begin by importing all the modules and setting the fixed random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from os.path import join\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import libs.cylib as cylib\n",
    "from data import Dataset\n",
    "\n",
    "tf.set_random_seed(31415)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The `Dataset` class implements an iterator which returns the next batch data in each iteration. Data is already normalized to have zero mean and unit variance. The iteration is terminated when we reach the end of the data (one epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2975, 160, 384, 3)\n",
      "Validation shape: (500, 160, 384, 3)\n",
      "mean =  [-0.03841928 -0.04215339 -0.05894543]\n",
      "std =  [ 1.01808784  1.02022915  1.02732566]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "# create the Dataset for training and validation\n",
    "train_data = Dataset('train', batch_size)\n",
    "val_data = Dataset('val', batch_size, shuffle=False)\n",
    "\n",
    "print('Train shape:', train_data.x.shape)\n",
    "print('Validation shape:', val_data.x.shape)\n",
    "\n",
    "print('mean = ', train_data.x.mean((0,1,2)))\n",
    "print('std = ', train_data.x.std((0,1,2)))\n",
    "\n",
    "# store the input image dimensions\n",
    "height = train_data.height\n",
    "width = train_data.width\n",
    "channels = train_data.channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "First, we will create input placeholders for Tensorflow computational graph of the model. For a supervised learning model, we need to declare placeholders which will hold input images (x) and target labels (y) of the mini-batches as\n",
    "we feed them to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create placeholders for inputs\n",
    "def build_inputs():\n",
    "    with tf.name_scope('data'):\n",
    "        x = tf.placeholder(tf.float32, shape=(None, height, width, channels), name='rgb_images')\n",
    "        y = tf.placeholder(tf.int32, shape=(None, height, width), name='labels')\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Now we can define the computational graph. Here we will heavily use [`tf.layers`](https://www.tensorflow.org/api_docs/python/tf/layers) high level API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv():\n",
    "    \n",
    "def block(size):\n",
    "    \n",
    "def build_model(x, num_classes):\n",
    "    input_size = x.get_shape().as_list()[1:3]\n",
    "    maps = [64, 64, 64, 64]\n",
    "    skip_layers = []\n",
    "    x = conv(x, 32, k=5)\n",
    "    x = pool(x)\n",
    "    x = conv(x, maps[0])\n",
    "    # x = conv(x, maps[0])\n",
    "    skip_layers.append(x)\n",
    "    x = pool(x)\n",
    "    x = conv(x, maps[1])\n",
    "    # x = conv(x, maps[1])\n",
    "    skip_layers.append(x)\n",
    "    x = pool(x)\n",
    "    x = conv(x, maps[2])\n",
    "    # x = conv(x, maps[2])\n",
    "    skip_layers.append(x)\n",
    "    x = pool(x)\n",
    "    x = conv(x, maps[3])\n",
    "    # x = conv(x, maps[3])\n",
    "\n",
    "    logits = tf.layers.conv2d(x, num_classes, 1, padding='same')\n",
    "\n",
    "    logits = tf.image.resize_bilinear(logits, input_size, name='upsample_logits')\n",
    "    return logits, is_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, y):\n",
    "  with tf.name_scope('loss'):\n",
    "    y = tf.reshape(y, shape=[-1])\n",
    "    logits = tf.reshape(logits, [-1, num_classes])\n",
    "\n",
    "    mask = y < num_classes\n",
    "    y = tf.boolean_mask(y, mask)\n",
    "    logits = tf.boolean_mask(logits, mask)\n",
    "\n",
    "    y_one_hot = tf.one_hot(y, num_classes)\n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_one_hot)\n",
    "\n",
    "    xent = tf.reduce_mean(xent)\n",
    "    tf.summary.scalar('cross_entropy', xent)\n",
    "    loss = add_regularization(xent)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4438f9f365d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_model' is not defined"
     ]
    }
   ],
   "source": [
    "x, y = build_inputs()\n",
    "logits, is_training = build_model(x, num_classes)\n",
    "loss = build_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Restoring a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Tensor(\"Relu_8:0\", shape=(?, 10, 24, 128), dtype=float32) \n",
      " Tensor(\"Relu_6:0\", shape=(?, 20, 48, 256), dtype=float32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-204185088070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'build_loss' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
