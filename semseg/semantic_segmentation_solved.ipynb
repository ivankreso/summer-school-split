{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "In this exercise we will train an end-to-end convolutional neural network for semantic segmentation.\n",
    "The goal of semantic segmentation is to classify the image on the pixel level, for each pixel\n",
    "we want to determine the class of the object to which it belongs. This is different from image classification\n",
    "which classifies an image as a whole and doesn't tell us the location of the objects. This is why semantic segmentation goes into the category of [structured prediction](https://en.wikipedia.org/wiki/Structured_prediction) problems. It answers on both the 'what' and 'where' questions while classifcation tells us only 'what'.\n",
    "\n",
    "Input image | Target image\n",
    "-|-\n",
    "![raw](assets/frankfurt_000000_014480.png) | ![raw](assets/frankfurt_000000_014480_labels.png)\n",
    "![raw](assets/frankfurt_000001_005898.png) | ![raw](assets/frankfurt_000001_005898_labels.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cityscapes dataset\n",
    "\n",
    "[Cityscapes dataset](https://www.cityscapes-dataset.com/dataset-overview/) contains a diverse set of stereo video sequences recorded in street scenes from 50 different cities, with high quality pixel-level annotations. Dataset contains 2975 training and 500 validation images of size 2048x1024. The test set of 1000 images is evaluated on the server and benchmark is available [here](https://www.cityscapes-dataset.com/benchmarks/#scene-labeling-task). Here we will use downsampled images of size 384x160. The original dataset has 19 classes but we lowered that to 7 by uniting similar classes. This makes sense due to low visibility of very small objects in downsampled images. We also have ignore class which we need to ignore during training.\n",
    "\n",
    "* Download the prepared dataset [here]() and extract it to the current directory. \n",
    "\n",
    "ID | Class | Color\n",
    "-|-|-\n",
    "0 | road | purple\n",
    "1 | building | grey\n",
    "2 | infrastructure | yellow\n",
    "3 | nature | green\n",
    "4 | sky | light blue\n",
    "5 | person | red\n",
    "6 | vehicle | dark blue\n",
    "7 | ignore | black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the graph\n",
    "\n",
    "Let's begin by importing all the modules and setting the fixed random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA5FJREFUeJzt1MENwCAQwLDS/Xc+tgCJ2BPklTUzHwDv+28HAHCG4QNE\nGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QY\nPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+\nQIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5A\nhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE\n4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QITh\nA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOED\nRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNE\nGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QY\nPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+\nQIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5A\nhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE\n4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QITh\nA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOED\nRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNE\nGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QYPkCE4QNEGD5AhOEDRBg+QIThA0QY\nPkCE4QNEGD5AhOEDRBg+QMQGL4sE9RSocXsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f234e156f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from os.path import join\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import utils\n",
    "from data import Dataset\n",
    "\n",
    "tf.set_random_seed(31415)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The `Dataset` class implements an iterator which returns the next batch data in each iteration. Data is already normalized to have zero mean and unit variance. The iteration is terminated when we reach the end of the dataset (one epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2975, 160, 384, 3)\n",
      "Validation shape: (500, 160, 384, 3)\n",
      "mean =  [ 73.40336636  83.00610775  72.3497469 ]\n",
      "std =  [ 47.74221113  48.59657488  47.74085072]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "num_classes = Dataset.num_classes\n",
    "# create the Dataset for training and validation\n",
    "train_data = Dataset('train', batch_size)\n",
    "val_data = Dataset('val', batch_size, shuffle=False)\n",
    "\n",
    "print('Train shape:', train_data.x.shape)\n",
    "print('Validation shape:', val_data.x.shape)\n",
    "\n",
    "print('mean = ', train_data.x.mean((0,1,2)))\n",
    "print('std = ', train_data.x.std((0,1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "First, we will create input placeholders for Tensorflow computational graph of the model. For a supervised learning model, we need to declare placeholders which will hold input images (x) and target labels (y) of the mini-batches as we feed them to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store the input image dimensions\n",
    "height = train_data.height\n",
    "width = train_data.width\n",
    "channels = train_data.channels\n",
    "\n",
    "# create placeholders for inputs\n",
    "def build_inputs():\n",
    "    with tf.name_scope('data'):\n",
    "        x = tf.placeholder(tf.float32, shape=(None, height, width, channels), name='rgb_images')\n",
    "        y = tf.placeholder(tf.int32, shape=(None, height, width), name='labels')\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Now we can define the computational graph. Here we will heavily use [`tf.layers`](https://www.tensorflow.org/api_docs/python/tf/layers) high level API which handles `tf.Variable` creation for us. The main difference here compared to the classification model is that the network is going to be fully convolutional without any fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function which applies conv2d + ReLU with filter size k\n",
    "def conv(x, num_maps, k=3):\n",
    "    x = tf.layers.conv2d(x, num_maps, k, padding='same')\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "# helper function for 2x2 max pooling with stride=2\n",
    "def pool(x):\n",
    "    return tf.layers.max_pooling2d(x, pool_size=2, strides=2, padding='same')\n",
    "\n",
    "# this functions takes the input placeholder and the number of classes, builds the model and returns the logits\n",
    "def build_model(x, num_classes):\n",
    "    input_size = x.get_shape().as_list()[1:3]\n",
    "    block_sizes = [64, 64, 64, 64]\n",
    "    x = conv(x, 32, k=3)\n",
    "    for i, size in enumerate(block_sizes):\n",
    "        with tf.name_scope('block'+str(i)):\n",
    "            x = pool(x)\n",
    "            x = conv(x, size)\n",
    "            x = conv(x, size)\n",
    "    print(x)\n",
    "    with tf.name_scope('logits'):\n",
    "        x = tf.layers.conv2d(x, num_classes, 1, padding='same')\n",
    "        x = tf.image.resize_bilinear(x, input_size, name='upsample_logits')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "Now we are going to implement the `build_loss` funcion which will create operations for loss computation and return the final `tf.Tensor` holding the scalar loss value.\n",
    "Because segmentation is just classification on a pixel level we can again use the cross entropy loss function \\\\(L\\\\) between the target one-hot distribution \\\\( \\mathbf{y} \\\\) and the predicted distribution from a softmax layer \\\\( \\mathbf{s} \\\\). But compared to image clasificaion here we need to define the loss at each pixel. Below are the equations describing the loss for one example (one pixel in our case).\n",
    "$$\n",
    "L = - \\sum_{i=1}^{C} y_i log(s_j(\\mathbf{x})) \\\\\n",
    "s_i(\\mathbf{x}) = \\frac{e^{x_i}}{\\sum_{j=1}^{C} e^{x_j}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this funcions takes logits and targets (y) and builds the loss subgraph\n",
    "def build_loss(logits, y):\n",
    "  with tf.name_scope('loss'):\n",
    "    # vectorize the image\n",
    "    y = tf.reshape(y, shape=[-1])\n",
    "    logits = tf.reshape(logits, [-1, num_classes])\n",
    "    \n",
    "    # gather all labels with valid ID\n",
    "    mask = y < num_classes\n",
    "    y = tf.boolean_mask(y, mask)\n",
    "    logits = tf.boolean_mask(logits, mask)\n",
    "\n",
    "    y_one_hot = tf.one_hot(y, num_classes)\n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_one_hot)\n",
    "\n",
    "    xent = tf.reduce_mean(xent)\n",
    "    tf.summary.scalar('cross_entropy', xent)\n",
    "    return xent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Now we can use all the building blocks from above and construct the whole Tensorflow graph in just a couple of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"block3/Relu_1:0\", shape=(?, 10, 24, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# create inputs\n",
    "x, y = build_inputs()\n",
    "# create model\n",
    "logits = build_model(x, num_classes)\n",
    "# create loss\n",
    "loss = build_loss(logits, y)\n",
    "# we are going to need argmax predictions for IoU\n",
    "y_pred = tf.argmax(logits, axis=3, output_type=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "We usually evaluate the semantic segmentation results with [Intersection over Union](https://en.wikipedia.org/wiki/Jaccard_index) measure (IoU aka Jaccard index). Note that accurracy we used on MNIST image classification problem is a bad measure in this case because semantic segmentation datasets are often heavily imbalanced. First we compute IoU for each class in one-vs-all fashion (shown below) and then take the mean IoU (mIoU) over all classes. By taking the mean we are treating all classes as equally important.\n",
    "$$\n",
    "IOU = \\frac{TP}{TP + FN + FP}\n",
    "$$\n",
    "\n",
    "![iou](assets/iou.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(sess, data, x, y, y_pred, loss, draw_steps=0):\n",
    "    print('\\nValidation phase:')\n",
    "    conf_mat = np.zeros((num_classes, num_classes), dtype=np.uint64) \n",
    "    for i, (x_np, y_np, names) in enumerate(data):\n",
    "        start_time = time.time()\n",
    "        loss_np, y_pred_np = sess.run([loss, y_pred],\n",
    "          feed_dict={x: x_np, y: y_np})\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        batch_conf_mat = confusion_matrix(y_np.reshape(-1), y_pred_np.reshape(-1))\n",
    "        batch_conf_mat = batch_conf_mat[:-1,:-1].astype(np.uint64)\n",
    "        conf_mat += batch_conf_mat\n",
    "\n",
    "        for j in range(min(draw_steps, batch_size)):\n",
    "            img_pred = utils.colorize_labels(y_pred_np[j], Dataset.class_info)\n",
    "            img_true = utils.colorize_labels(y_np[j], Dataset.class_info)\n",
    "            img_raw = data.get_img(names[j])\n",
    "            img = np.concatenate((img_raw, img_true, img_pred), axis=1)\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            draw_steps -= 1\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            string = 'batch %03d loss = %.2f  (%.1f images/sec)' % \\\n",
    "            (i, loss_np, x_np.shape[0] / duration)\n",
    "            print(string)\n",
    "    print(conf_mat)\n",
    "    return utils.print_stats(conf_mat, 'Validation', Dataset.class_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, x, y, y_pred, loss, checkpoint_dir):\n",
    "    num_epochs = 30\n",
    "    batch_size = 10\n",
    "    log_dir = 'local/logs'\n",
    "    utils.clear_dir(log_dir)\n",
    "    utils.clear_dir(checkpoint_dir)\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    decay_power = 1.0\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    decay_steps = num_epochs * train_data.num_batches\n",
    "\n",
    "    lr = tf.train.polynomial_decay(learning_rate, global_step, decay_steps,\n",
    "                                   end_learning_rate=0, power=decay_power)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    summary_all = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(join(log_dir, 'train'), sess.graph)\n",
    "\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "\n",
    "    step = 0\n",
    "    best_iou = 0\n",
    "    best_epoch = 0\n",
    "    exp_start_time = time.time()\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # confusion_mat = np.zeros((num_classes, num_classes), dtype=np.uint64)\n",
    "        print('\\nTraining phase:')\n",
    "        for x_np, y_np, names in train_data:\n",
    "            start_time = time.time()\n",
    "            loss_np, summary, _ = sess.run([loss, summary_all, train_step],\n",
    "            feed_dict={x: x_np, y: y_np})\n",
    "            train_writer.add_summary(summary, step)\n",
    "            duration = time.time() - start_time\n",
    "            # confusion_mat += batch_conf_mat.astype(np.uint64)\n",
    "            if step % 20 == 0:\n",
    "                string = '%s: epoch %d / %d, iter %05d, loss = %.2f  (%.1f images/sec)' % \\\n",
    "                (utils.get_expired_time(exp_start_time), epoch, num_epochs, step, loss_np, batch_size / duration)\n",
    "                print(string)\n",
    "            step += 1\n",
    "        # utils.print_metrics(confusion_mat, 'Train') \n",
    "        iou = validate(sess, val_data, x, y, y_pred, loss, draw_steps=0)\n",
    "        if iou > best_iou:\n",
    "            best_iou, best_epoch = iou, epoch\n",
    "            save_path = saver.save(sess, join(checkpoint_dir, 'model.ckpt'))\n",
    "            print('Model saved in file: ', save_path)\n",
    "        print('\\nBest IoU = %.2f (epoch %d)' % (best_iou, best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training phase:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (10, 160, 384, 3) for Tensor 'data/labels:0', which has shape '(?, 160, 384)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0b0a7b1a773a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'local/checkpoint1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-252c6621a51a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, x, y, y_pred, loss, checkpoint_dir)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             loss_np, summary, _ = sess.run([loss, summary_all, train_step],\n\u001b[0;32m---> 36\u001b[0;31m             feed_dict={x: x_np, y: y_np})\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1101\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (10, 160, 384, 3) for Tensor 'data/labels:0', which has shape '(?, 160, 384)'"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "train(sess, x, y, y_pred, loss, 'local/checkpoint1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Restoring the pretrained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore the best checkpoint\n",
    "checkpoint_path = 'local/pretrained1/model.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, checkpoint_path)\n",
    "validate(sess, val_data, x, y, y_pred, loss, draw_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Improved model with skip connections\n",
    "\n",
    "In this part we are going to improve on the pravious model by adding skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upsample(x, skip, num_maps):\n",
    "    skip_size = skip.get_shape().as_list()[1:3]\n",
    "    x = tf.image.resize_bilinear(x, skip_size)\n",
    "    x = tf.concat([x, skip], 3)\n",
    "    return conv(x, num_maps)\n",
    "\n",
    "# this functions takes the input placeholder and the number of classes, builds the model and returns the logits\n",
    "def build_model(x, num_classes):\n",
    "    input_size = x.get_shape().as_list()[1:3]\n",
    "    block_sizes = [64, 64, 64, 64]\n",
    "    skip_layers = []\n",
    "    \n",
    "    x = conv(x, 32, k=3)\n",
    "    for i, size in enumerate(block_sizes):\n",
    "        with tf.name_scope('block'+str(i)):\n",
    "            x = pool(x)\n",
    "            x = conv(x, size)\n",
    "            x = conv(x, size)\n",
    "            skip_layers.append(x)\n",
    "    \n",
    "    for i, skip in reversed(list(enumerate(skip_layers))):\n",
    "        with tf.name_scope('block'+str(i)):\n",
    "            print(i, x, '\\n', skip)\n",
    "            x = upsample(x, skip, block_sizes[i])\n",
    "    \n",
    "    with tf.name_scope('logits'):\n",
    "        x = tf.layers.conv2d(x, num_classes, 1, padding='same')\n",
    "        x = tf.image.resize_bilinear(x, input_size, name='upsample_logits')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# create inputs\n",
    "x, y = build_inputs()\n",
    "# create model\n",
    "logits = build_model(x, num_classes)\n",
    "# create loss\n",
    "loss = build_loss(logits, y)\n",
    "# we are going to need argmax predictions for IoU\n",
    "y_pred = tf.argmax(logits, axis=3, output_type=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "train(sess, x, y, y_pred, loss, 'local/checkpoint2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore the best checkpoint\n",
    "checkpoint_path = 'local/pretrained2/model.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, checkpoint_path)\n",
    "validate(sess, val_data, x, y, y_pred, loss, draw_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Homework\n",
    "\n",
    "If you wish you can play with the model. Try to improve on the current IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
