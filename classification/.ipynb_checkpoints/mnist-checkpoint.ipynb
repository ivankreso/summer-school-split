{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a basic TensorFlow tutorial on image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "Contains images of handwritten digits (0, 1, 2, ..., 9).\n",
    "\n",
    "Download the dataset using TF's built-in method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every MNIST sample has two parts:\n",
    "an image (vectorized, raster-scanned) of a handwritten digit and a corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_sample(index):\n",
    "    image = mnist.train.images[index].reshape(28, 28) # 784 -> 28x28\n",
    "    label = mnist.train.labels[index]\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(image, cmap='Greys')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    print('label[%d]: %s' % (index, str(label)))\n",
    "\n",
    "show_sample(10)\n",
    "show_sample(24)\n",
    "show_sample(12)\n",
    "show_sample(11)\n",
    "show_sample(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Our classification model\n",
    "\n",
    "We're going to train a model to look at images and predict what digits they are.\n",
    "\n",
    "A function $M: \\mathbb{R}^{28\\times 28}\\rightarrow \\mathbb{R}^{10}$ outputs a classification score for each input digit.\n",
    "In other words, $M(\\text{image})=\\text{a vector of per-class scores}$.\n",
    "We want that a higher score for class $c$ translates to higher confidence that $c$ is the correct class.\n",
    "\n",
    "For example, if $M$ outputs\n",
    "$$\n",
    "    (0.05, 0.03, 0.82, 0.02, 0.01, 0.02, 0.01, 0.02, 0.01, 0.1)\n",
    "$$\n",
    "for an input image, it classifies that image as a $2$.\n",
    "\n",
    "Let us choose a very simple classification model first:\n",
    "$$\n",
    "    M(\\mathbf{x})=\n",
    "    \\mathbf{x}\\cdot\\mathbf{W} + \\mathbf{b}\n",
    "    ,\n",
    "$$\n",
    "where $\\mathbf{x}\\in\\mathbb{R}^{784}$ is a vectorized input image, and $\\mathbf{W}\\in\\mathbb{R}^{784\\times 10}$ and $\\mathbf{b}\\in\\mathbb{R}^{10}$ are the model parameters. The elements of $M(\\mathbf{x})$ are sometimes called **logits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# `x` is a batch of input images (each reshaped into a vector)\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# define the model\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "logits = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the model parameters from data\n",
    "\n",
    "Initially, $\\mathbf{W}$ and $\\mathbf{b}$ contain random values that will not produce correct classification results.\n",
    "\n",
    "We have to tune these tensors by minimizing an appropirate loss function that will \"measure\" the quality of classification.\n",
    "\n",
    "We will use the **cross entropy criterion**:\n",
    "$$\n",
    "    L(\\mathbf{x}, c)=\n",
    "    -\\log p_c(\\mathbf{x})\n",
    "    ,\n",
    "$$\n",
    "where $p_c(\\mathbf{x})$ is the **probability** assigned by the model that $\\mathbf{x}$ belongs to class $c$,\n",
    "$$\n",
    "    p_c=\n",
    "    \\frac{e^{l_c}}{\\sum_{j=1}^{10} e^{l_j}}\n",
    "    ,\n",
    "$$\n",
    "and $(l_0, l_1, \\ldots, l_9)=M(\\mathbf{x})$ are the logits output by the model.\n",
    "\n",
    "The derivatives can now be computed by TensorFlow and the model can be tuned with **stochastic gradient descent** ($k=0, 1, 2, \\ldots$):\n",
    "$$\n",
    "    \\mathbf{W}_{k+1}=\n",
    "    \\mathbf{W}_k - \\eta\\frac{\\partial L}{\\partial\\mathbf{W}_k}\n",
    "$$\n",
    "$$\n",
    "    \\mathbf{b}_{k+1}=\n",
    "    \\mathbf{b}_k - \\eta\\frac{\\partial L}{\\partial\\mathbf{b}_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare the loss function (`labels` are denoted as `c` in the text above)\n",
    "labels = tf.placeholder(tf.float32, [None, 10])\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "# we will use SGD to learn the model\n",
    "step = tf.train.GradientDescentOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss $L$ is usually approximated on a batch of images.\n",
    "The code above can handle this case as well.\n",
    "We set the batch size to $100$ is our experiment.\n",
    "\n",
    "We measure the quality of the model on a separate testing dataset by counting the number of images that it has correctly classified:\n",
    "$$\n",
    "    \\text{accuracy}=\n",
    "    \\frac{\\text{number of correctly classified samples}}{\\text{total number of samples}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Befor starting the learning process, we must properly initialize TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(5000):\n",
    "    batch_xs, batch_labels = mnist.train.next_batch(100)\n",
    "    sess.run(step, feed_dict={x: batch_xs, labels: batch_labels})\n",
    "    if k % 200 == 0:\n",
    "        acc = 100*sess.run(accuracy, feed_dict={x: mnist.test.images, labels: mnist.test.labels})\n",
    "        print('* iter %d: test set accuracy=%.2f %%' % (k, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
